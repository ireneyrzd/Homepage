+++
date = "2021-07-26T12:00:00-00:00"
title = "MusicNet Brainstorm and Analysis"
tags = [ "Machine Learning Brainstorms" ]

+++

If you look at my other post (SRA Experience Recap 7/26), the current music generation models can be  improved to become more customizable if a dataset with specific labels are created. Then, when a user prompts for a certain characteristic as input, only songs with corresponding tags would be fed into the training model, and the output result would follow the desire of the user.

##### Possible Labels to the MusicNet datasets
* Composer
* Musical period (eg. Baroque, Classical, Romantic, Contemporary)
* Genre/style (eg. dance, sonata, nocturne, jazz, blue, pop)
* Speed
* Difficulty (ranked based on standards of different exams, eg. Certificate of Merit, Associated Board of the Royal Schools of Music)
* Mood that it evokes

I looked at three papers that explore how  music can evoke senses of emotion in audiences from different backgrounds.
### "Ooh là là! Music evokes at least 13 emotions. Scientists have mapped them"
https://news.berkeley.edu/2020/01/06/music-evokes-13-emotions/

In the study, the Berkeley doctoral students found that the emotions evoked by music can be categorized into 13 feelings: "Amusement, joy, eroticism, beauty, relaxation, sadness, dreaminess, triumph, anxiety, scariness, annoyance, defiance, and feeling pumped up. "

In addition, the study found that while people from different regions experience similar emotions while listening to the same music, they classified these emotions differently. However, the Chinese and U.S. study participants differed in opinion on whether the emotions, for example "fear", evokes a positive or negative feeling.

Finally, the study proves that the emotion evoked by the music is not influenced whether the person associates it with a certain movie, as a similar research conducted using Chinese traditional music reflects similar results.



### "Music and emotion"
https://en.wikipedia.org/wiki/Music_and_emotion

The field of music and emotion is a branch of music psychology that studies the relationship of music and emotion in the human brain. The end target is to discover how music therapy can be efficiently applied to people from a variety of backgrounds.

The subject seeks how the influence of music changes with the age of the audience. How different aspects of music, such as tempo, mode, loudness, melody, and rhythm, can evoke different reactions. The Juslin & Västfjäll's BRECVEM model found that music can elicit emotion through seven ways: brain stem reflex, rhythmic entrainment, evaluative conditioning, emotional contagion, episodic memory, and musical expectancy.

Researchers are able study the changes in emotion through self-reports or outsider's observation of listeners' physiological responses and expressive behavior both conveyed and elicited.

Finally, better understanding of the relationship between music and relationships can help create more effective therapeutic tools using music. Music therapy already shows promising outputs in treating patients with autism, the hope is that in the future, it would help more people, such as those with other mental illnesses, or others trying to break alcohol or drug addiction.

With better understanding of how music can be used as therapeutic tools and classification of what type of music is the most effective, the hope is that we can then utilize computer-generated music to create a variety of playlists with specific attributes targeting a group of people with a specific therapeutic need.
